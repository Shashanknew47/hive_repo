

This is a real time dataset of the ineuron technical consultant team. You have to perform hive analysis on this given dataset.

Download Dataset 1 - https://drive.google.com/file/d/1WrG-9qv6atP-W3P_-gYln1hHyFKRKMHP/view

Download Dataset 2 - https://drive.google.com/file/d/1-JIPCZ34dyN6k9CqJa-Y8yxIGq6vTVXU/view

Note: both files are csv files.


1. Create a schema based on the given dataset
2. Dump the data inside the hdfs in the given schema location.
3. List of all agents' names.
4. Find out agent average rating.

  SELECT avg(rating) as avg_rating FROM agent_performance;
    avg_rating
    1.4609629649255012


5. Total working days for each agents
     30 days
    with unique_table as (SELECT distinct agent_name, date FROM agent_performance)
    > SELECT agent_name, count(date) FROM unique_table;



6. Total query that each agent have taken
     Note : considering  each chat as a query
     SELECT agent_name, sum(total_chats) FROM agent_performance Group by agent_name;



7. Total Feedback that each agent have received
    SELECT agent_name, sum(feedback) FROM agent_performance Group by agent_name;


8. Agent name who have average rating between 3.5 to 4
    SELECT agent_name,rating FROM agent_performance Where rating between 3.5 and 4;


9. Agent name who have rating less than 3.5
    SELECT agent_name FROM agent_performance Where rating < 3.5;


10. Agent name who have rating more than 4.5
    SELECT agent_name FROM agent_performance Where rating > 4.5;


11. How many feedback agents have received more than 4.5 average
      SELECT sum(feedback) FROM agent_performance WHERE feedback > 4.5;
          8976


12. average weekly response time for each agent




13. average weekly resolution time for each agents





14. Find the number of chat on which they have received a feedback
        => This should be total number of feedbacks
             SELECT sum(feedback) as total_feedback FROM agent_performance;
                9259




15. Total contribution hour for each and every agents weekly basis





16. Perform inner join, left join and right join based on the agent column and after joining the table export that data into your local system.




17. Perform partitioning on top of the agent column and then on top of that perform bucketing for each partitioning.





Best way to Export Hive table to CSV file
This post is to explain different options available to export Hive Table (ORC, Parquet or Text) to CSV File.

Expected output : CSV File with comma delimiter and header

Method 1 :

hive -e 'select * from table_orc_data;' | sed 's/[[:space:]]\+/,/g' > ~/output.csv

    Pros : Simple to use, output column header but default output is tab.
    Cons : Need to convert Tab delimiter to ‘,’ which could be time consuming when exporting large file.

Method 2:

$ hadoop fs -cat hdfs://servername/user/hive/warehouse/databasename/table_csv_export_data/* > ~/output.csvCopy

    Pros : Simple, with comma as delimiter in CSV output.
    Cons : No column headers.

Method 3: (My personal favorite)

— Step 3a: Create CSV table with dummy header column as first row.

CREATE TABLE table_csv_export_data
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
STORED as textfile
AS
select
'id' as id
,'first_name' as first_name
,'last_name' as last_name
,'join_date' as join_date;

— Step 3b: Now insert data actual data into table

INSERT INTO table_csv_export_data
SELECT
 id
 ,first_name
 ,last_name
 ,join_date
FROM
 table_orc_data;

— Step 3c

hadoop fs -cat hdfs://servername/user/hive/warehouse/databasename/table_csv_export_data/* > ~/output.csv

Pros : CSV with header columns
Cons : Extra line of script to add header info as row, but final output is quick and as desired.

If you like the article feel free to clap here and follow me on https://twitter.com/gchandra

Source : http://www.gchandra.com/hadoop/hive-hadoop/best-way-to-export-hive-table-to-csv-file.html
