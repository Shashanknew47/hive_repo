

This is a real time dataset of the ineuron technical consultant team. You have to perform hive analysis on this given dataset.

Download Dataset 1 - https://drive.google.com/file/d/1WrG-9qv6atP-W3P_-gYln1hHyFKRKMHP/view

Download Dataset 2 - https://drive.google.com/file/d/1-JIPCZ34dyN6k9CqJa-Y8yxIGq6vTVXU/view

Note: both files are csv files.


1. Create a schema based on the given dataset
       CREATE TABLE agent_performance_partition
    >   (
    >      id int,
    >      date string,
    >      total_chats int,
    >      average_response_time string,
    >      average_resolution_time string,
    >      rating float,
    >      feedback int
    >      )

    row format DELIMITED
    fields TERMINATED by ',';



2. Dump the data inside the hdfs in the given schema location.





3. List of all agents' names.
    SELECT agent_name FROM agent_performance_part_bucket

4. Find out agent average rating.

  SELECT avg(rating) as avg_rating FROM agent_performance_part_bucket;
    avg_rating
    1.4609629649255012


5. Total working days for each agents
     30 days


    with unique_table as (SELECT distinct agent_name, date FROM agent_performance_part_bucket)
    > SELECT agent_name, count(date) FROM unique_table;



6. Total query that each agent have taken
     Note : considering  each chat as a query
     SELECT agent_name, sum(total_chats) FROM agent_performance Group by agent_name;




7. Total Feedback that each agent have received
    SELECT agent_name, sum(feedback) FROM agent_performance Group by agent_name;


8. Agent name who have average rating between 3.5 to 4
    SELECT agent_name,rating FROM agent_performance Where rating between 3.5 and 4;


9. Agent name who have rating less than 3.5
    SELECT agent_name FROM agent_performance Where rating < 3.5;


10. Agent name who have rating more than 4.5
    SELECT agent_name FROM agent_performance Where rating > 4.5;


11. How many feedback agents have received more than 4.5 average
      SELECT sum(feedback) FROM agent_performance WHERE feedback > 4.5;
          8976




CREATE TABLE agent_performance
    (
        id int,
        week int,
        response_time int,
        resolution_time int
    )
    stored as orc;

=========================================================================================
    > INSERT INTO agent_support
    > select transform(id,date,average_response_time,average_resolution_time)
    > using 'python final.py' as (id int,week int, response_time int, resolution_time int)
    > FROM agent_performance;

=============================================================================================

 SELECT p.id,p.agent_name,p.date,s.week,s.response_time,s.resolution_time
     FROM agent_performance as p join agent_support as s
     on p.id = s.id
     limit 50;

==========================================================================




12. average weekly response time for each agent

    SELECT
    agent_name,
    sum(r_time)/ count(agent_name) as average_agent_week_response_time


    FROM
     ( SELECT
            p.agent_name,s.week,sum(s.response_time) as r_time
        FROM
                agent_performance as p join agent_support as s
                on p.id = s.id
        GROUP BY
            p.agent_name,
            s.week ) as week_r_time

    GROUP BY
        agent_name



13. average weekly resolution time for each agents

    SELECT
    agent_name,
    sum(r_time)/ count(agent_name) as average_agent_week_response_time


    FROM
     ( SELECT
            p.agent_name,s.week,sum(s.resolution_time) as r_time
        FROM
                agent_performance as p join agent_support as s
                on p.id = s.id
        GROUP BY
            p.agent_name,
            s.week ) as week_r_time

    GROUP BY
        agent_name





14. Find the number of chat on which they have received a feedback
        => This should be total number of feedbacks
             SELECT sum(feedback) as total_feedback FROM agent_performance;
                9259




15. Total contribution hour for each and every agents weekly basis






16. Perform inner join, left join and right join based on the agent column and after joining the table export that data into your local system.








17. Perform partitioning on top of the agent column and then on top of that perform bucketing for each partitioning.


        CREATE TABLE agent_performance_partition
    >   (
    >      id int,
    >      date string,
    >      total_chats int,
    >      average_response_time string,
    >      average_resolution_time string,
    >      rating float,
    >      feedback int
    >      )
    >   partitioned by (agent_name string);


       hive.exec.dynamic.partition.mode=nonstrict;

       INSERT OVERWRITE TABLE agent_performance_partition PARTITION(agent_name)
       SELECT id, date, total_chats, average_response_time, average_resolution_time, rating, feedback,agent_name
        FROM agent_performance;




        CREATE TABLE agent_performance_partition
    >   (
    >      id int,
    >      date string,
    >      total_chats int,
    >      average_response_time string,
    >      average_resolution_time string,
    >      rating float,
    >      feedback int,
           agent_name
    >      )
    >      clustered by (id)
    >      sorted by (id)
           into 3 buckets;

     INSERT OVERWRITE table agent_performance_part_bucket SELECT * FROM agent_performance_partition;






Best way to Export Hive table to CSV file
This post is to explain different options available to export Hive Table (ORC, Parquet or Text) to CSV File.

Expected output : CSV File with comma delimiter and header

Method 1 :

hive -e 'select * from table_orc_data;' | sed 's/[[:space:]]\+/,/g' > ~/output.csv

    Pros : Simple to use, output column header but default output is tab.
    Cons : Need to convert Tab delimiter to ‘,’ which could be time consuming when exporting large file.

Method 2:

$ hadoop fs -cat hdfs://servername/user/hive/warehouse/databasename/table_csv_export_data/* > ~/output.csvCopy

    Pros : Simple, with comma as delimiter in CSV output.
    Cons : No column headers.

Method 3: (My personal favorite)

— Step 3a: Create CSV table with dummy header column as first row.

CREATE TABLE table_csv_export_data
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
STORED as textfile
AS
select
'id' as id
,'first_name' as first_name
,'last_name' as last_name
,'join_date' as join_date;

— Step 3b: Now insert data actual data into table

INSERT INTO table_csv_export_data
SELECT
 id
 ,first_name
 ,last_name
 ,join_date
FROM
 table_orc_data;

— Step 3c

hadoop fs -cat hdfs://servername/user/hive/warehouse/databasename/table_csv_export_data/* > ~/output.csv

Pros : CSV with header columns
Cons : Extra line of script to add header info as row, but final output is quick and as desired.

If you like the article feel free to clap here and follow me on https://twitter.com/gchandra

Source : http://www.gchandra.com/hadoop/hive-hadoop/best-way-to-export-hive-table-to-csv-file.html
